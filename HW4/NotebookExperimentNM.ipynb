{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting RNN.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile RNN.py\n",
    "import numpy as np \n",
    "from FunctionGradient import SoftMaxFunc, tanh, tanhGradient, sigmoid, sigmoidGradient, ReLu, ReLuGradient\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "\n",
    "##output layer\n",
    "class SoftmaxLayer(object):\n",
    "    def __init__(self, rng, n_in, n_out):\n",
    "        ##initialize weight W_io:\n",
    "        self.W = np.asarray(rng.uniform(\n",
    "        low = -np.sqrt(6.0 / (n_in + n_out)),\n",
    "        high = np.sqrt(6.0 / (n_in + n_out)),\n",
    "            size = (n_in, n_out)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        ##initialize bias:\n",
    "        self.bias = np.asarray(rng.uniform(\n",
    "        low = -np.sqrt(6.0 / (n_out + n_out)),\n",
    "        high = np.sqrt(6.0 / (n_out + n_out)),\n",
    "            size = (n_out,)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "\n",
    "##hidden layer\n",
    "class HiddenLayer(object):\n",
    "    def __init__(self, rng, n_in, n_h, activation, activationGradient):\n",
    "        #initialize weight from input to hidden layer:\n",
    "        self.W_ih = np.asarray( rng.uniform(\n",
    "            low = -np.sqrt(6.0 / (n_in + n_h)),\n",
    "            high = np.sqrt(6.0 / (n_in + n_h)),\n",
    "            size = (n_in, n_h)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        #initialize weight from hidden layer to hidden layer: \n",
    "        self.W_hh = np.asarray( rng.uniform(\n",
    "            low = -np.sqrt(6.0 / (n_h + n_h)),\n",
    "            high = np.sqrt(6.0 / (n_h + n_h)),\n",
    "            size = (n_h, n_h)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        ##initialize bias:\n",
    "        self.bias = np.asarray(rng.uniform(\n",
    "        low = -np.sqrt(6.0 / (n_h + n_h)),\n",
    "        high = np.sqrt(6.0 / (n_h + n_h)),\n",
    "            size = (n_h,)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        #pre state\n",
    "        self.preHidden = np.zeros((n_h,))\n",
    "        \n",
    "        #activation function and its corresponding gradient function:\n",
    "        self.activation = activation\n",
    "        self.activationGradient = activationGradient\n",
    "        \n",
    "#RNN:network       \n",
    "class RNNNet(object):\n",
    "    def __init__(self, rng, n_in, n_h, n_out, T = 10, \\\n",
    "        activation = tanh, activationGradient = tanhGradient, learningRate = 10**(-6)):\n",
    "        #update weight every sequence of length T\n",
    "        self.T = T;\n",
    "        self.t = 0;\n",
    "        self.hiddenLayer = HiddenLayer(\n",
    "            rng = rng, n_in = n_in, n_h = n_h, activation = activation, activationGradient = activationGradient\n",
    "        )\n",
    "\n",
    "        self.softmaxLayer = SoftmaxLayer(\n",
    "            rng = rng, n_in = n_h, n_out = n_out\n",
    "        )\n",
    "        self.learningRate = learningRate\n",
    "        \n",
    "        ##store the intermediate states of length T\n",
    "        #delta_k^t:\n",
    "        self.delta_k_t = np.zeros((T,n_out));\n",
    "        #a_h^t: a_h at time t (3.30)\n",
    "        self.a_h_t = np.zeros((T,n_h))\n",
    "        #b_h^t: b_h at time t (3.31)\n",
    "        self.b_h_t = np.zeros((T,n_h))\n",
    "        #delta_h_t, delta_h_(T + 1) = 0\n",
    "        self.delta_h_t = np.zeros((T + 1,n_h))\n",
    "        #x_t: input at time t\n",
    "        self.x_t = np.zeros((T, 256))\n",
    "        #previous hidden state at time t\n",
    "        self.preHidden = np.zeros((T, n_h))\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, inputValue, output):\n",
    "        t = self.t;\n",
    "        self.x_t[t,:] = inputValue\n",
    "        self.preHidden[t,:] = self.hiddenLayer.preHidden;\n",
    "\n",
    "        #propagate to the hidden layer, bias corresponds to a unit with constant ouput 1\n",
    "        #(3.30)\n",
    "        linearOutput = np.dot(inputValue, self.hiddenLayer.W_ih) \\\n",
    "            + np.dot(self.hiddenLayer.preHidden, self.hiddenLayer.W_hh) \\\n",
    "            + self.hiddenLayer.bias\n",
    "            \n",
    "        self.a_h_t[t,:] = linearOutput\n",
    "       \n",
    "        #(3.31)  \n",
    "        y = self.hiddenLayer.activation(linearOutput)\n",
    "        self.b_h_t[t,:] = y\n",
    "        self.hiddenLayer.preHidden = y\n",
    "\n",
    "\n",
    "        #propagate to the top layer\n",
    "        linearOutput = np.dot(y, self.softmaxLayer.W) + \\\n",
    "                                               self.softmaxLayer.bias\n",
    "        y = SoftMaxFunc(linearOutput)\n",
    "        #predict = np.argmax(y)\n",
    "\n",
    "        delta = output - y\n",
    "        self.delta_k_t[t,:] = delta\n",
    "        \n",
    "        t = t + 1\n",
    "        self.t = t;\n",
    "        if (t == self.T) :\n",
    "            self.backward()\n",
    "            self.t = 0\n",
    "\n",
    "    #backward and updates the weight every T characters\n",
    "    def backward(self):\n",
    "        #get sequence of \\delta_h^t (3.33)\n",
    "        for t in range(self.T - 1, -1,-1):\n",
    "            self.delta_h_t[t,:] = np.multiply(self.hiddenLayer.activationGradient(self.a_h_t[t,:]),\\\n",
    "                np.dot(self.softmaxLayer.W,self.delta_k_t[t,:]) + \\\n",
    "                np.dot(self.hiddenLayer.W_hh,self.delta_h_t[t + 1,:]))\n",
    "        \n",
    "        for t in range(0,self.T,1):\n",
    "            #update W_hk, and biase_hk (3.35)\n",
    "            self.softmaxLayer.W =  self.softmaxLayer.W + self.learningRate*\\\n",
    "                np.outer(self.b_h_t[t,:], self.delta_k_t[t,:])\n",
    "            self.softmaxLayer.bias = self.softmaxLayer.bias + \\\n",
    "                                     self.learningRate*self.delta_k_t[t,:]\n",
    "            \n",
    "            #update W_ih, biase_ih\n",
    "            self.hiddenLayer.W_ih = self.hiddenLayer.W_ih + self.learningRate*\\\n",
    "                np.outer(self.x_t[t,:], self.delta_h_t[t,:])\n",
    "            self.hiddenLayer.bias =  self.hiddenLayer.bias + \\\n",
    "                                     self.learningRate*self.delta_h_t[t,:];\n",
    "            \n",
    "            #update W_hh\n",
    "            self.hiddenLayer.W_hh = self.hiddenLayer.W_hh + self.learningRate*\\\n",
    "                np.outer(self.preHidden[t,:], self.delta_h_t[t,:])\n",
    "    \n",
    "    \n",
    "    #pass the input and generate the output, do not update the weights and initial state\n",
    "    #of original RNN\n",
    "    def onePass(self, preHidden, inputValue):\n",
    "        linearOutput = np.dot(inputValue, self.hiddenLayer.W_ih) \\\n",
    "            + np.dot(preHidden, self.hiddenLayer.W_hh) \\\n",
    "            + self.hiddenLayer.bias\n",
    "            \n",
    "        y = self.hiddenLayer.activation(linearOutput)\n",
    "        preHidden = y;\n",
    "        linearOutput = np.dot(y, self.softmaxLayer.W) + \\\n",
    "                                            self.softmaxLayer.bias\n",
    "        y = SoftMaxFunc(linearOutput)\n",
    "        return (y, preHidden)\n",
    "            \n",
    "        \n",
    "    #train the network\n",
    "    def train(self, INPUT):\n",
    "        for i in range(len(INPUT) - 1):\n",
    "            currChar = INPUT[i,:]\n",
    "            nextChar = INPUT[i + 1,:]\n",
    "            self.forward(currChar, nextChar);\n",
    "    \n",
    "    #compute training loss\n",
    "    def trainingLoss(self, INPUT):\n",
    "        trainLoss = 0;\n",
    "        preHidden = self.hiddenLayer.preHidden\n",
    "        \n",
    "        for i in range(len(INPUT) - 1):\n",
    "            currChar = INPUT[i,:]\n",
    "            nextChar = INPUT[i + 1,:]\n",
    "            (y, preHidden) = self.onePass(preHidden,currChar)\n",
    "            trainLoss = trainLoss - np.dot(nextChar,np.log(y))\n",
    "            \n",
    "        return trainLoss\n",
    "    \n",
    "    ##sample a charater according the probability of the output\n",
    "    def sample(self, f):\n",
    "        #f: pdf\n",
    "        n = len(f)\n",
    "        #F: CDF\n",
    "        F = np.zeros((n,))\n",
    "        F[0] = f[0]\n",
    "        \n",
    "        for i in range(1,n):\n",
    "            F[i] = f[i] + F[i - 1]\n",
    "        \n",
    "        randomNum = np.random.uniform(0,1)\n",
    "        return np.searchsorted(F, randomNum)\n",
    "    \n",
    "    \n",
    "    #sampling the text: start from the 'start' character and \n",
    "    #generate a sequence of character with legnth (lenght + 1)\n",
    "    def test(self, start, length):\n",
    "        sequence = []\n",
    "        sequence.append(unichr(start));\n",
    "        \n",
    "        inputValue = np.zeros((256,))\n",
    "        inputValue[start] = 1;\n",
    "        \n",
    "        preHidden = self.hiddenLayer.preHidden\n",
    "        prevInput = start;\n",
    "        \n",
    "        for i in range(1,length + 1):\n",
    "            (y, preHidden) = self.onePass(preHidden,inputValue)\n",
    "            nextInput = self.sample(y)\n",
    "            \n",
    "            \n",
    "            #next character by sampling\n",
    "            sequence.append(unichr(nextInput))\n",
    "            inputValue[prevInput] = 0\n",
    "            inputValue[nextInput] = 1\n",
    "            \n",
    "            prevInput = nextInput\n",
    "            \n",
    "        return sequence\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.searchsorted([2,3,4,7],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1  0.2  0.3  0.9  1. ]\n",
      "0.278415408994\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def sample(f):\n",
    "    n = len(f)\n",
    "    F = np.zeros((n,))\n",
    "        \n",
    "    F[0] = f[0]\n",
    "        \n",
    "    for i in range(1,n):\n",
    "        F[i] = f[i] + F[i - 1]\n",
    "        \n",
    "    randomNum = np.random.uniform(0,1)\n",
    "    print F\n",
    "    print randomNum\n",
    "        \n",
    "    return np.searchsorted(F, randomNum)\n",
    "\n",
    "print sample([0.1,0.1,0.1,0.6,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.388260285442808"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = []\n",
    "a.append('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
